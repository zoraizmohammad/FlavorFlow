{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1seTo1s-2X_pBUHPLzoEU_A_YiN3-4ytk",
      "authorship_tag": "ABX9TyPGgZtyh8pIZNE+wrgaHgfr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoraizmohammad/FlavorFlow/blob/main/data/dataProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Processing**"
      ],
      "metadata": {
        "id": "bykNqSiMNdBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Insertion**"
      ],
      "metadata": {
        "id": "W9s_9xTqw_RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "View Tasks here: https://github.com/zoraizmohammad/FlavorFlow/blob/main/weeklyTasks/week2.md#to-complete-step-1-data-collection-and-preparation"
      ],
      "metadata": {
        "id": "RECcTRaKwPHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-4i1YbjNy9P",
        "outputId": "d2ae828c-1f91-4cbe-b3a2-fdee62fca47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN - Has all imports\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import words, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from quantulum3 import parser\n",
        "from fuzzywuzzy import fuzz, process\n",
        "import re"
      ],
      "metadata": {
        "id": "QjXdhtjK0gJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def view(file_path, num_columns):\n",
        "  df = pd.read_csv(file_path)\n",
        "  print(df.iloc[:, :num_columns])"
      ],
      "metadata": {
        "id": "sX5Mu-luxYYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_annotation = pd.read_csv('/content/drive/MyDrive/FlavorSync/Data/full_dataset.csv')\n",
        "# Based on who's using copy and paste in path. Refrence:\n",
        "# RP MZ: /content/drive/MyDrive/FlavorSync/Data/full_dataset.csv\n",
        "# RP GM:\n",
        "# RP RM:"
      ],
      "metadata": {
        "id": "BEyGSVnuPcUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_annotation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "hUjjhfX5P3X8",
        "outputId": "bacc17f9-b159-4f6d-a5a3-953776e70ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Unnamed: 0                                          title  \\\n",
              "0                 0                            No-Bake Nut Cookies   \n",
              "1                 1                          Jewell Ball'S Chicken   \n",
              "2                 2                                    Creamy Corn   \n",
              "3                 3                                  Chicken Funny   \n",
              "4                 4                           Reeses Cups(Candy)     \n",
              "...             ...                                            ...   \n",
              "2231137     2231137                            Sunny's Fake Crepes   \n",
              "2231138     2231138                                     Devil Eggs   \n",
              "2231139     2231139  Extremely Easy and Quick - Namul Daikon Salad   \n",
              "2231140     2231140     Pan-Roasted Pork Chops With Apple Fritters   \n",
              "2231141     2231141                 Polpette in Spicy Tomato Sauce   \n",
              "\n",
              "                                               ingredients  \\\n",
              "0        [\"1 c. firmly packed brown sugar\", \"1/2 c. eva...   \n",
              "1        [\"1 small jar chipped beef, cut up\", \"4 boned ...   \n",
              "2        [\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...   \n",
              "3        [\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...   \n",
              "4        [\"1 c. peanut butter\", \"3/4 c. graham cracker ...   \n",
              "...                                                    ...   \n",
              "2231137  [\"1/2 cup chocolate hazelnut spread (recommend...   \n",
              "2231138  [\"1 dozen eggs\", \"1 paprika\", \"1 salt and pepp...   \n",
              "2231139  [\"150 grams Daikon radish\", \"1 tbsp Sesame oil...   \n",
              "2231140  [\"1 cup apple cider\", \"6 tablespoons sugar\", \"...   \n",
              "2231141  [\"1 pound ground veal\", \"1/2 pound sweet Itali...   \n",
              "\n",
              "                                                directions  \\\n",
              "0        [\"In a heavy 2-quart saucepan, mix brown sugar...   \n",
              "1        [\"Place chipped beef on bottom of baking dish....   \n",
              "2        [\"In a slow cooker, combine all ingredients. C...   \n",
              "3        [\"Boil and debone chicken.\", \"Put bite size pi...   \n",
              "4        [\"Combine first four ingredients and press in ...   \n",
              "...                                                    ...   \n",
              "2231137  [\"Spread hazelnut spread on 1 side of each tor...   \n",
              "2231138  [\"Boil eggs on medium for 30mins.\", \"Then cool...   \n",
              "2231139  [\"Julienne the daikon and squeeze out the exce...   \n",
              "2231140  [\"In a large bowl, mix the apple cider with 4 ...   \n",
              "2231141  [\"Preheat the oven to 350.\", \"In a bowl, mix t...   \n",
              "\n",
              "                                                      link     source  \\\n",
              "0           www.cookbooks.com/Recipe-Details.aspx?id=44874   Gathered   \n",
              "1          www.cookbooks.com/Recipe-Details.aspx?id=699419   Gathered   \n",
              "2           www.cookbooks.com/Recipe-Details.aspx?id=10570   Gathered   \n",
              "3          www.cookbooks.com/Recipe-Details.aspx?id=897570   Gathered   \n",
              "4          www.cookbooks.com/Recipe-Details.aspx?id=659239   Gathered   \n",
              "...                                                    ...        ...   \n",
              "2231137  www.foodnetwork.com/recipes/sunny-anderson/sun...  Recipes1M   \n",
              "2231138           cookpad.com/us/recipes/355411-devil-eggs  Recipes1M   \n",
              "2231139  cookpad.com/us/recipes/153324-extremely-easy-a...  Recipes1M   \n",
              "2231140                cooking.nytimes.com/recipes/1015164  Recipes1M   \n",
              "2231141  www.foodandwine.com/recipes/polpette-spicy-tom...  Recipes1M   \n",
              "\n",
              "                                                       NER  \n",
              "0        [\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...  \n",
              "1        [\"beef\", \"chicken breasts\", \"cream of mushroom...  \n",
              "2        [\"frozen corn\", \"cream cheese\", \"butter\", \"gar...  \n",
              "3        [\"chicken\", \"chicken gravy\", \"cream of mushroo...  \n",
              "4        [\"peanut butter\", \"graham cracker crumbs\", \"bu...  \n",
              "...                                                    ...  \n",
              "2231137  [\"chocolate hazelnut spread\", \"tortillas\", \"bu...  \n",
              "2231138  [\"eggs\", \"paprika\", \"salt\", \"choice\", \"miracle...  \n",
              "2231139  [\"radish\", \"Sesame oil\", \"White sesame seeds\",...  \n",
              "2231140  [\"apple cider\", \"sugar\", \"kosher salt\", \"bay l...  \n",
              "2231141  [\"ground veal\", \"sausage\", \"bread crumbs\", \"mi...  \n",
              "\n",
              "[2231142 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e947bb2d-61de-416d-af11-d74ad7e8f3ec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>ingredients</th>\n",
              "      <th>directions</th>\n",
              "      <th>link</th>\n",
              "      <th>source</th>\n",
              "      <th>NER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>No-Bake Nut Cookies</td>\n",
              "      <td>[\"1 c. firmly packed brown sugar\", \"1/2 c. eva...</td>\n",
              "      <td>[\"In a heavy 2-quart saucepan, mix brown sugar...</td>\n",
              "      <td>www.cookbooks.com/Recipe-Details.aspx?id=44874</td>\n",
              "      <td>Gathered</td>\n",
              "      <td>[\"brown sugar\", \"milk\", \"vanilla\", \"nuts\", \"bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Jewell Ball'S Chicken</td>\n",
              "      <td>[\"1 small jar chipped beef, cut up\", \"4 boned ...</td>\n",
              "      <td>[\"Place chipped beef on bottom of baking dish....</td>\n",
              "      <td>www.cookbooks.com/Recipe-Details.aspx?id=699419</td>\n",
              "      <td>Gathered</td>\n",
              "      <td>[\"beef\", \"chicken breasts\", \"cream of mushroom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Creamy Corn</td>\n",
              "      <td>[\"2 (16 oz.) pkg. frozen corn\", \"1 (8 oz.) pkg...</td>\n",
              "      <td>[\"In a slow cooker, combine all ingredients. C...</td>\n",
              "      <td>www.cookbooks.com/Recipe-Details.aspx?id=10570</td>\n",
              "      <td>Gathered</td>\n",
              "      <td>[\"frozen corn\", \"cream cheese\", \"butter\", \"gar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Chicken Funny</td>\n",
              "      <td>[\"1 large whole chicken\", \"2 (10 1/2 oz.) cans...</td>\n",
              "      <td>[\"Boil and debone chicken.\", \"Put bite size pi...</td>\n",
              "      <td>www.cookbooks.com/Recipe-Details.aspx?id=897570</td>\n",
              "      <td>Gathered</td>\n",
              "      <td>[\"chicken\", \"chicken gravy\", \"cream of mushroo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Reeses Cups(Candy)</td>\n",
              "      <td>[\"1 c. peanut butter\", \"3/4 c. graham cracker ...</td>\n",
              "      <td>[\"Combine first four ingredients and press in ...</td>\n",
              "      <td>www.cookbooks.com/Recipe-Details.aspx?id=659239</td>\n",
              "      <td>Gathered</td>\n",
              "      <td>[\"peanut butter\", \"graham cracker crumbs\", \"bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2231137</th>\n",
              "      <td>2231137</td>\n",
              "      <td>Sunny's Fake Crepes</td>\n",
              "      <td>[\"1/2 cup chocolate hazelnut spread (recommend...</td>\n",
              "      <td>[\"Spread hazelnut spread on 1 side of each tor...</td>\n",
              "      <td>www.foodnetwork.com/recipes/sunny-anderson/sun...</td>\n",
              "      <td>Recipes1M</td>\n",
              "      <td>[\"chocolate hazelnut spread\", \"tortillas\", \"bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2231138</th>\n",
              "      <td>2231138</td>\n",
              "      <td>Devil Eggs</td>\n",
              "      <td>[\"1 dozen eggs\", \"1 paprika\", \"1 salt and pepp...</td>\n",
              "      <td>[\"Boil eggs on medium for 30mins.\", \"Then cool...</td>\n",
              "      <td>cookpad.com/us/recipes/355411-devil-eggs</td>\n",
              "      <td>Recipes1M</td>\n",
              "      <td>[\"eggs\", \"paprika\", \"salt\", \"choice\", \"miracle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2231139</th>\n",
              "      <td>2231139</td>\n",
              "      <td>Extremely Easy and Quick - Namul Daikon Salad</td>\n",
              "      <td>[\"150 grams Daikon radish\", \"1 tbsp Sesame oil...</td>\n",
              "      <td>[\"Julienne the daikon and squeeze out the exce...</td>\n",
              "      <td>cookpad.com/us/recipes/153324-extremely-easy-a...</td>\n",
              "      <td>Recipes1M</td>\n",
              "      <td>[\"radish\", \"Sesame oil\", \"White sesame seeds\",...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2231140</th>\n",
              "      <td>2231140</td>\n",
              "      <td>Pan-Roasted Pork Chops With Apple Fritters</td>\n",
              "      <td>[\"1 cup apple cider\", \"6 tablespoons sugar\", \"...</td>\n",
              "      <td>[\"In a large bowl, mix the apple cider with 4 ...</td>\n",
              "      <td>cooking.nytimes.com/recipes/1015164</td>\n",
              "      <td>Recipes1M</td>\n",
              "      <td>[\"apple cider\", \"sugar\", \"kosher salt\", \"bay l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2231141</th>\n",
              "      <td>2231141</td>\n",
              "      <td>Polpette in Spicy Tomato Sauce</td>\n",
              "      <td>[\"1 pound ground veal\", \"1/2 pound sweet Itali...</td>\n",
              "      <td>[\"Preheat the oven to 350.\", \"In a bowl, mix t...</td>\n",
              "      <td>www.foodandwine.com/recipes/polpette-spicy-tom...</td>\n",
              "      <td>Recipes1M</td>\n",
              "      <td>[\"ground veal\", \"sausage\", \"bread crumbs\", \"mi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2231142 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e947bb2d-61de-416d-af11-d74ad7e8f3ec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e947bb2d-61de-416d-af11-d74ad7e8f3ec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e947bb2d-61de-416d-af11-d74ad7e8f3ec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-014d2e00-47dc-4bd5-9a80-32c26ecac87a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-014d2e00-47dc-4bd5-9a80-32c26ecac87a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-014d2e00-47dc-4bd5-9a80-32c26ecac87a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3d5d6818-2d64-4c58-8a5a-397c43994f0e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('raw_annotation')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3d5d6818-2d64-4c58-8a5a-397c43994f0e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('raw_annotation');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "raw_annotation"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Cleaning & Export**"
      ],
      "metadata": {
        "id": "LVySkbCkwZYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/FlavorSync/Data/full_dataset.csv')\n",
        "\n",
        "#Remove Duplicates\n",
        "# Filter by source and remove duplicate recipes\n",
        "data = data[data['source'] == \"Gathered\"].drop_duplicates()\n",
        "\n",
        "# Normalize Text\n",
        "# Download necessary NLTK data\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Create a lemmatizer instance\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Base ingredient names\n",
        "base_ingredients = [\n",
        "    \"bell pepper\", \"chili\", \"cilantro\", \"eggplant\", \"zucchini\", \"potato\", \"tomato\",\n",
        "    \"onion\", \"garlic\", \"carrot\", \"mushroom\", \"cabbage\", \"lettuce\", \"spinach\"\n",
        "]\n",
        "\n",
        "# Ingredient mapping dictionary\n",
        "ingredient_mapping = {ingredient: ingredient for ingredient in base_ingredients}\n",
        "\n",
        "# Function to automate ingredient mapping using fuzzy matching\n",
        "def create_ingredient_mapping(ingredient_list, base_ingredients, threshold=80):\n",
        "    for ingredient in ingredient_list:\n",
        "        # Find the best match from the base ingredients using fuzzy matching\n",
        "        match, score = process.extractOne(ingredient, base_ingredients)\n",
        "        if score >= threshold:  # If the similarity score is above the threshold\n",
        "            ingredient_mapping[ingredient] = match\n",
        "\n",
        "# Get a list of unique ingredients from the dataset\n",
        "unique_ingredients = set()\n",
        "data['ingredients'].apply(lambda x: unique_ingredients.update(word_tokenize(x.lower())))\n",
        "\n",
        "# Generate ingredient mappings\n",
        "create_ingredient_mapping(unique_ingredients, base_ingredients)\n",
        "\n",
        "# Function to normalize ingredients\n",
        "def normalize_ingredients(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase the text\n",
        "    normalized_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        # Lemmatize each token and map using the ingredient dictionary\n",
        "        lemmatized_word = lemmatizer.lemmatize(token)\n",
        "        if lemmatized_word in ingredient_mapping:\n",
        "            normalized_tokens.append(ingredient_mapping[lemmatized_word])\n",
        "        else:\n",
        "            normalized_tokens.append(lemmatized_word)\n",
        "\n",
        "    return ' '.join(normalized_tokens)\n",
        "\n",
        "# Apply normalization to the 'ingredients' column\n",
        "data['ingredients'] = data['ingredients'].apply(normalize_ingredients)\n",
        "\n",
        "#Spell-Check and Correct Typos\n",
        "english_words = set(words.words())\n",
        "\n",
        "def correct_spelling(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    corrected_tokens = [token if token in english_words else token for token in tokens]\n",
        "    return ' '.join(corrected_tokens)\n",
        "\n",
        "# Apply spell-check to the 'steps' column\n",
        "data['steps'] = data['steps'].apply(correct_spelling)\n",
        "\n",
        "# Standardize Units\n",
        "def standardize_units(text):\n",
        "    quantities = parser.parse(text)\n",
        "    standardized_text = \" \".join([str(quantity) for quantity in quantities])\n",
        "    return standardized_text\n",
        "\n",
        "# Apply unit standardization\n",
        "data['ingredients'] = data['ingredients'].apply(standardize_units)\n",
        "\n",
        "# Save the cleaned data\n",
        "data.to_csv(\"/content/drive/MyDrive/FlavorSync/Data/cleaned_data.csv\", index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "id": "NknUlbVivKTa",
        "outputId": "a9f28f60-2dab-4687-e128-2c93ea3e3c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-47c006711bf4>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Get a list of unique ingredients from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0munique_ingredients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ingredients'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munique_ingredients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Generate ingredient mappings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-47c006711bf4>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Get a list of unique ingredients from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0munique_ingredients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ingredients'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0munique_ingredients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Generate ingredient mappings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenize Ingredients & Steps**"
      ],
      "metadata": {
        "id": "fXj4aRuzySjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/FlavorSync/Data/cleaned_data.csv\")\n",
        "\n",
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 1: Ingredients Tokenization\n",
        "def tokenize_ingredient(ingredient):\n",
        "    # Regular expression to capture quantity, unit, and ingredient name\n",
        "    pattern = r'(?P<quantity>\\d*\\.?\\d+)?\\s*(?P<unit>\\b\\w+\\b)?\\s*(?P<ingredient>.+)'\n",
        "    match = re.match(pattern, ingredient)\n",
        "\n",
        "    if match:\n",
        "        quantity = match.group('quantity') if match.group('quantity') else None\n",
        "        unit = match.group('unit') if match.group('unit') else None\n",
        "        ingredient_name = match.group('ingredient').strip() if match.group('ingredient') else None\n",
        "        return {'quantity': quantity, 'unit': unit, 'ingredient': ingredient_name}\n",
        "    return {'quantity': None, 'unit': None, 'ingredient': ingredient}\n",
        "\n",
        "# Apply the tokenization to the 'ingredients' column\n",
        "data['ingredients_tokenized'] = data['ingredients'].apply(lambda x: [tokenize_ingredient(ing) for ing in x.split(',')])\n",
        "\n",
        "# Step 2: Steps Tokenization\n",
        "# Define common cooking actions\n",
        "cooking_actions = [\n",
        "    \"chop\", \"saute\", \"bake\", \"fry\", \"boil\", \"mix\", \"stir\", \"grill\", \"roast\",\n",
        "    \"slice\", \"dice\", \"mince\", \"blend\", \"whisk\", \"season\", \"cook\", \"serve\"\n",
        "]\n",
        "\n",
        "def tokenize_step(step):\n",
        "    # Tokenize the step into words\n",
        "    tokens = word_tokenize(step.lower())\n",
        "\n",
        "    # Find actions and associated ingredients\n",
        "    actions = [action for action in cooking_actions if action in tokens]\n",
        "    related_ingredients = [token for token in tokens if token not in cooking_actions]\n",
        "\n",
        "    return {'actions': actions, 'related_ingredients': related_ingredients}\n",
        "\n",
        "# Apply the tokenization to the 'steps' column\n",
        "data['steps_tokenized'] = data['steps'].apply(tokenize_step)\n",
        "\n",
        "# Save the tokenized data\n",
        "data.to_csv(\"/content/drive/MyDrive/FlavorSync/Data/tokenized_data.csv\", index=False)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "-7AX33URyedY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Markov Chain State Modeling**"
      ],
      "metadata": {
        "id": "0YMhO_1_yyA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenized data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/FlavorSync/Data/tokenized_data.csv\")\n",
        "\n",
        "# Step 1: Assign State IDs to Ingredients\n",
        "# Flatten all ingredients into a single list\n",
        "all_ingredients = set()\n",
        "for ingredient_list in data['ingredients_tokenized']:\n",
        "    ingredient_list = eval(ingredient_list)  # Convert string representation to list of dicts\n",
        "    for item in ingredient_list:\n",
        "        if item['ingredient']:\n",
        "            all_ingredients.add(item['ingredient'])\n",
        "\n",
        "# Create a mapping of ingredients to state IDs\n",
        "ingredient_states = {ingredient: idx for idx, ingredient in enumerate(sorted(all_ingredients), start=1)}\n",
        "\n",
        "# Step 2: Assign State IDs to Cooking Actions\n",
        "# Extract all unique cooking actions from the steps\n",
        "all_actions = set()\n",
        "for step in data['steps_tokenized']:\n",
        "    step_data = eval(step)  # Convert string representation to dict\n",
        "    for action in step_data['actions']:\n",
        "        all_actions.add(action)\n",
        "\n",
        "# Create a mapping of actions to state IDs\n",
        "action_states = {action: idx for idx, action in enumerate(sorted(all_actions), start=1)}\n",
        "\n",
        "# Step 3: Create Mappings for Ingredients and Steps\n",
        "# Assign state IDs to each ingredient in the 'ingredients_tokenized' column\n",
        "def assign_ingredient_states(ingredient_list):\n",
        "    ingredient_list = eval(ingredient_list)\n",
        "    return [ingredient_states[item['ingredient']] for item in ingredient_list if item['ingredient']]\n",
        "\n",
        "data['ingredient_states'] = data['ingredients_tokenized'].apply(assign_ingredient_states)\n",
        "\n",
        "# Assign state IDs to each action in the 'steps_tokenized' column\n",
        "def assign_action_states(step_data):\n",
        "    step_data = eval(step_data)\n",
        "    return [action_states[action] for action in step_data['actions']]\n",
        "\n",
        "data['action_states'] = data['steps_tokenized'].apply(assign_action_states)\n",
        "\n",
        "# Save the data with state IDs\n",
        "data.to_csv(\"/content/drive/MyDrive/FlavorSync/Data/states_data.csv\", index=False)\n",
        "\n",
        "# Display the state mappings for reference\n",
        "ingredient_states, action_states\n"
      ],
      "metadata": {
        "id": "RWSrCFLTy7g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Annotate Data with Cuisines and Dietary Tags**"
      ],
      "metadata": {
        "id": "TSXUIPTIzE0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data with state IDs\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/FlavorSync/Data/states_data.csv\")\n",
        "\n",
        "# Expanded keyword-based rules for cuisine tagging\n",
        "cuisine_keywords = {\n",
        "    \"Italian\": [\"spaghetti\", \"pasta\", \"marinara\", \"mozzarella\", \"risotto\", \"parmesan\", \"basil\", \"gnocchi\", \"focaccia\"],\n",
        "    \"Mexican\": [\"taco\", \"quesadilla\", \"guacamole\", \"salsa\", \"enchilada\", \"jalapeno\", \"chipotle\", \"mole\", \"tamale\"],\n",
        "    \"Indian\": [\"curry\", \"masala\", \"paneer\", \"tandoori\", \"naan\", \"dal\", \"biryani\", \"samosa\", \"chutney\", \"ghee\"],\n",
        "    \"Chinese\": [\"soy sauce\", \"noodles\", \"dumpling\", \"fried rice\", \"tofu\", \"szechuan\", \"hoisin\", \"wonton\", \"dim sum\"],\n",
        "    \"French\": [\"baguette\", \"croissant\", \"ratatouille\", \"brie\", \"crepes\", \"coq au vin\", \"bouillabaisse\", \"quiche\"],\n",
        "    \"Japanese\": [\"sushi\", \"sashimi\", \"miso\", \"udon\", \"ramen\", \"teriyaki\", \"wasabi\", \"tempura\", \"matcha\", \"yakitori\"],\n",
        "    \"Thai\": [\"coconut milk\", \"lemongrass\", \"curry paste\", \"pad thai\", \"tom yum\", \"fish sauce\", \"basil\", \"sticky rice\"],\n",
        "    \"Greek\": [\"feta\", \"tzatziki\", \"gyro\", \"souvlaki\", \"moussaka\", \"olives\", \"spanakopita\", \"dolma\", \"baklava\"],\n",
        "    \"Middle Eastern\": [\"hummus\", \"falafel\", \"shawarma\", \"tahini\", \"sumac\", \"pita\", \"za'atar\", \"labneh\", \"tabbouleh\"],\n",
        "    \"Spanish\": [\"paella\", \"tapas\", \"chorizo\", \"gazpacho\", \"saffron\", \"manchego\", \"tortilla\", \"jamón\", \"patatas bravas\"],\n",
        "    \"Korean\": [\"kimchi\", \"bulgogi\", \"gochujang\", \"bibimbap\", \"kimbap\", \"doenjang\", \"samgyeopsal\", \"japchae\", \"soju\"],\n",
        "    \"Vietnamese\": [\"pho\", \"banh mi\", \"spring roll\", \"nuoc cham\", \"fish sauce\", \"lemongrass\", \"rice paper\", \"vermicelli\"],\n",
        "    \"Caribbean\": [\"jerk\", \"plantain\", \"callaloo\", \"ackee\", \"curry goat\", \"rum\", \"coconut\", \"conch\", \"pigeon peas\"],\n",
        "    \"Ethiopian\": [\"injera\", \"doro wat\", \"berbere\", \"shiro\", \"kitfo\", \"niter kibbeh\", \"teff\", \"lentils\", \"collard greens\"],\n",
        "    \"Moroccan\": [\"couscous\", \"tagine\", \"harissa\", \"ras el hanout\", \"preserved lemon\", \"mint tea\", \"saffron\", \"dates\"],\n",
        "    \"Turkish\": [\"kebab\", \"baklava\", \"lokum\", \"borek\", \"simit\", \"manti\", \"hummus\", \"dolma\", \"raki\", \"yogurt\"],\n",
        "    \"Brazilian\": [\"feijoada\", \"pão de queijo\", \"brigadeiro\", \"açai\", \"chimichurri\", \"tapioca\", \"farofa\", \"guarana\"],\n",
        "    \"Filipino\": [\"adobo\", \"sinigang\", \"lumpia\", \"halo-halo\", \"lechon\", \"pandesal\", \"tocino\", \"kare-kare\", \"balut\"],\n",
        "    \"Russian\": [\"borscht\", \"pelmeni\", \"blini\", \"kvass\", \"beetroot\", \"smetana\", \"pirozhki\", \"caviar\", \"vodka\"],\n",
        "    \"German\": [\"bratwurst\", \"sauerkraut\", \"pretzel\", \"schnitzel\", \"spätzle\", \"strudel\", \"bier\", \"mustard\"],\n",
        "    \"African\": [\"jollof rice\", \"fufu\", \"egusi\", \"koki\", \"bunny chow\", \"sosatie\", \"couscous\", \"baobab\", \"grilled fish\"],\n",
        "    \"American\": [\"burger\", \"hot dog\", \"BBQ\", \"mac and cheese\", \"fried chicken\", \"apple pie\", \"pancakes\", \"cornbread\"],\n",
        "    \"British\": [\"fish and chips\", \"shepherd's pie\", \"yorkshire pudding\", \"scones\", \"bangers and mash\", \"custard\"],\n",
        "    \"Australian\": [\"vegemite\", \"lamington\", \"barbie\", \"pavlova\", \"meat pie\", \"tim tam\", \"kangaroo\", \"anzac biscuit\"]\n",
        "}\n",
        "\n",
        "# Expanded keyword-based rules for dietary tagging\n",
        "dietary_keywords = {\n",
        "    \"Vegan\": [\"tofu\", \"tempeh\", \"seitan\", \"plant-based\", \"vegan\", \"nutritional yeast\", \"jackfruit\", \"lentils\"],\n",
        "    \"Vegetarian\": [\"cheese\", \"egg\", \"paneer\", \"vegetarian\", \"yogurt\", \"butter\", \"honey\", \"milk\"],\n",
        "    \"Gluten-Free\": [\"gluten-free\", \"quinoa\", \"rice\", \"cornmeal\", \"almond flour\", \"buckwheat\", \"sorghum\", \"tapioca\"],\n",
        "    \"Keto\": [\"avocado\", \"bacon\", \"cheese\", \"almond flour\", \"low-carb\", \"butter\", \"cream\", \"olive oil\", \"zucchini\"],\n",
        "    \"Paleo\": [\"grass-fed\", \"wild-caught\", \"almond flour\", \"coconut\", \"honey\", \"sweet potato\", \"ghee\", \"bone broth\"],\n",
        "    \"Low-FODMAP\": [\"zucchini\", \"carrot\", \"banana\", \"potato\", \"quinoa\", \"cucumber\", \"spinach\", \"blueberries\"],\n",
        "    \"Pescatarian\": [\"salmon\", \"tuna\", \"shrimp\", \"mackerel\", \"fish\", \"seafood\", \"sardines\", \"trout\"],\n",
        "    \"Nut-Free\": [\"nut-free\", \"seed\", \"sunflower butter\", \"sesame\", \"pumpkin seeds\", \"nut allergy\"],\n",
        "    \"Dairy-Free\": [\"dairy-free\", \"almond milk\", \"coconut milk\", \"oat milk\", \"lactose-free\", \"soy milk\"],\n",
        "    \"Soy-Free\": [\"soy-free\", \"coconut aminos\", \"sunflower oil\", \"ghee\", \"olive oil\", \"fish oil\"],\n",
        "    \"Whole30\": [\"whole30\", \"cauliflower rice\", \"coconut oil\", \"avocado oil\", \"zoodles\", \"compliant\", \"dates\"],\n",
        "    \"Halal\": [\"halal\", \"zabiha\", \"permissible\", \"no pork\", \"no alcohol\"],\n",
        "    \"Kosher\": [\"kosher\", \"pareve\", \"no shellfish\", \"no pork\", \"kosher salt\", \"matzo\"]\n",
        "}\n",
        "\n",
        "# Function to tag cuisine based on keywords\n",
        "def tag_cuisine(ingredients_text):\n",
        "    for cuisine, keywords in cuisine_keywords.items():\n",
        "        if any(keyword.lower() in ingredients_text.lower() for keyword in keywords):\n",
        "            return cuisine\n",
        "    return \"Other\"\n",
        "\n",
        "# Function to tag dietary information based on keywords\n",
        "def tag_dietary(ingredients_text):\n",
        "    tags = []\n",
        "    for diet, keywords in dietary_keywords.items():\n",
        "        if any(keyword.lower() in ingredients_text.lower() for keyword in keywords):\n",
        "            tags.append(diet)\n",
        "    return tags if tags else [\"None\"]\n",
        "\n",
        "# Apply the tagging functions to the data\n",
        "data['cuisine'] = data['ingredients'].apply(tag_cuisine)\n",
        "data['dietary_tags'] = data['ingredients'].apply(tag_dietary)\n",
        "\n",
        "# Manual Verification: Extract a sample for verification\n",
        "sample_data = data.sample(10)  # Adjust the sample size as needed for verification\n",
        "\n",
        "# Save the annotated data\n",
        "data.to_csv(\"/content/drive/MyDrive/FlavorSync/Data/annotated_data.csv\", index=False)\n",
        "\n",
        "# Display the sample data for manual verification\n",
        "sample_data\n"
      ],
      "metadata": {
        "id": "xINpadKlzMnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataFrame Creation**\n",
        "\n",
        "Data Structure:\n",
        "- recipe_id: Unique identifier for each recipe.\n",
        "- ingredient: The name of the ingredient used in the recipe.\n",
        "- quantity: The amount of the ingredient, converted to a standard format.\n",
        "- unit: The unit of measurement for the ingredient.\n",
        "- step: List of actions associated with the recipe (e.g., chop, mix, bake).\n",
        "- cuisine: The cuisine classification of the recipe (e.g., Italian, Chinese).\n",
        "- nutrition: Placeholder for nutritional information (if available).\n",
        "- tags: List of dietary tags associated with the recipe (e.g., Vegan, Gluten-Free).\n",
        "\n",
        "Processing Assumptions:\n",
        "- Quantities are converted to numeric values and missing quantities are set to 0.\n",
        "- Units are standardized to a consistent format for modeling.\n",
        "- Nutrition information is currently unavailable and marked as 'unknown'.\n",
        "- Missing values in ingredients, units, or nutrition fields are handled appropriately."
      ],
      "metadata": {
        "id": "oUD4CS6LzvjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the annotated data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/FlavorSync/Data/annotated_data.csv\")\n",
        "\n",
        "# Step 1: Structure Data in a DataFrame\n",
        "# Extract unique recipe IDs\n",
        "data['recipe_id'] = data.index + 1  # Assign unique IDs based on the index\n",
        "\n",
        "# Split ingredients into separate components\n",
        "structured_data = []\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    ingredients_list = eval(row['ingredients_tokenized'])  # Convert string to list of dicts\n",
        "    steps_list = eval(row['steps_tokenized'])  # Convert string to dict\n",
        "    for ingredient in ingredients_list:\n",
        "        structured_data.append({\n",
        "            'recipe_id': row['recipe_id'],\n",
        "            'ingredient': ingredient['ingredient'],\n",
        "            'quantity': ingredient['quantity'],\n",
        "            'unit': ingredient['unit'],\n",
        "            'step': steps_list['actions'],\n",
        "            'cuisine': row['cuisine'],\n",
        "            'nutrition': None,  # Placeholder for nutrition info if available\n",
        "            'tags': row['dietary_tags']\n",
        "        })\n",
        "\n",
        "# Create a structured DataFrame\n",
        "df = pd.DataFrame(structured_data)\n",
        "\n",
        "# Step 2: Data Quality Checks\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "# Fill missing values or handle them appropriately\n",
        "df['quantity'].fillna(0, inplace=True)  # Assuming 0 for missing quantities\n",
        "df['unit'].fillna(\"unknown\", inplace=True)  # Placeholder for unknown units\n",
        "df['nutrition'].fillna(\"unknown\", inplace=True)  # Placeholder for missing nutrition info\n",
        "\n",
        "# Check for outliers in 'quantity'\n",
        "df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')  # Convert to numeric\n",
        "outliers = df[(df['quantity'] < 0) | (df['quantity'] > 1000)]  # Example check for outliers\n",
        "print(\"Outliers:\\n\", outliers)\n",
        "\n",
        "# Standardize units (if not done earlier)\n",
        "# Example: Converting 'tablespoon' to 'tbsp' or 'grams' to 'g'\n",
        "df['unit'] = df['unit'].replace({\n",
        "    'tablespoon': 'tbsp',\n",
        "    'teaspoon': 'tsp',\n",
        "    'kilogram': 'kg',\n",
        "    'liter': 'l',\n",
        "    'ounce': 'oz',\n",
        "    # Add more conversions as needed\n",
        "})\n",
        "\n",
        "# Step 3: Save Data\n",
        "# Export cleaned and structured data to a .csv file\n",
        "df.to_csv(\"/mnt/data/structured_data.csv\", index=False)\n",
        "\n",
        "# Display the first few rows of the structured DataFrame\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "utR3dbPGz5N0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}